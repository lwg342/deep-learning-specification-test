{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import utils \n","from importlib import reload  \n","reload(utils)\n","\n","N = 1000\n","num_repeat = 30000\n","\n","def true_output(x1, x2, x3, x4):\n","    # y = x1.pow(2) + 2* torch.sin(x2) + x1*x2 + x3 * x4\n","    y = x1 + 2* x2 + 3 * x3 + 4*x4\n","    return y\n","\n","\n","class Net(nn.Module):\n","    def __init__(self) -> None:\n","        super().__init__()\n","        self.f1 = nn.Linear(4, 5)\n","        self.f2 = nn.Linear(5, 5)\n","        self.f3 = nn.Linear(5, 5)\n","        self.f4 = nn.Linear(5, 5)\n","        self.predict = nn.Linear(5, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.f1(x))\n","        x = F.relu(self.f2(x))\n","        x = F.relu(self.f3(x))\n","        x = F.relu(self.f4(x))\n","        out = self.predict(x)\n","        return out\n","\n","\n","def repeat():\n","    \n","    rng = np.random.default_rng()\n","\n","\n","    x1 = torch.from_numpy(rng.normal(0, 1, N))\n","    x2 = torch.from_numpy(rng.normal(0, 1, N))\n","    x3 = torch.from_numpy(rng.normal(0, 1, N))\n","    x4 = torch.from_numpy(rng.normal(0, 1, N))\n","    e = 0.2*torch.randn(N)\n","\n","\n","    y = (true_output(x1, x2, x3, x4) + e).unsqueeze(-1).float()\n","    x1.unsqueeze(-1).float()\n","    x2.unsqueeze(-1).float()\n","    x3.unsqueeze(-1).float()\n","    x4.unsqueeze(-1).float()\n","    x = torch.stack((x1, x2, x3, x4)).transpose(0, 1).float()\n","\n","    net = Net()\n","    # print(net(x)[0:5])\n","    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n","    loss_func = torch.nn.MSELoss()  \n","    for t in range(num_repeat):\n","        prediction = net(x)     # input x and predict based on x\n","\n","        loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n","\n","\n","        optimizer.zero_grad()   # clear gradients for next train\n","        loss.backward()         # backpropagation, compute gradients\n","        optimizer.step()        # apply gradients\n","\n","        if t == 1:\n","            print(loss)\n","        if t == num_repeat-1:\n","            print(loss)\n","\n","    # Test the Escanciano method\n","    from wl_regression import OLS  \n","    z0 = OLS(x.numpy(), y.numpy()).y_hat()\n","    e0 = (z0 - y.detach().numpy()[:,0])\n","\n","    z = net(x)\n","    e1 = (z-y).detach().numpy()\n","\n","    # from wl_regression import loc_poly\n","    # ll_z = loc_poly(y.numpy(), x.numpy(), x.detach().numpy())\n","    # e2 = (ll_z - y.detach().numpy()[:,0])\n","\n","    C_resid = utils.C_resid\n","    C = C_resid(e0, e1, e0, N)\n","    # print(C)\n","\n","    test_statistic = utils.test_statistic\n","    rslt = test_statistic(C, N)\n","    \n","\n","    return rslt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rslt_repeat = []\n","for j in range(100):\n","    rslt_repeat += [repeat()]\n","print(rslt_repeat)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}